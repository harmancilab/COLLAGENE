## Federated Weighted K-means

In this example, we put together several ideas from previous examples and implement a federated weighted K-means algorithm. We would like to emphasize that this algorithm is presented here as a descriptive example.

# Scenario
Federated K-means comes up when multiple sites would like to cluster their datasets in the global setting of all data. The main justification for federated clustering becomes clear when the inherent clusters within the pooled data is distributed among sites such that each site has a portion of some clusters while some sites do not even have any data from some of the cluster.

*generate_data.R* script implements this by generating data for each site by simulation of clusters.

## Weighted K-Means

The generic K-means algorithm is used to fit K-centroids to a dataset by iteratively updating centroids. In each iteration, each data point is assigned to the closest centroid (using Eucledian distances). Next, the centroids are updated using the mean of the points that are assigned to them.

To implement this algorithm, it is necessary to identify the minimum distance of each point to all centroids and select data points, which is computationally challenging in HE. There is also the possibility that some centroids may not be assigned any data points and should be removed from calculations. 

We modify the general K-means with a weighted version where each point is "softly" assigned to a centroid using a softmax-like score. At each iteration, we use:
```
w(a_i, c_j)=exp(-|a_i-c_j|^2/5)
```
Next, we update the jth centroid after weighting the coordinates of all points:
```
c_j=\sum_i w(a_i, c_j).a_i / \sum_i w(a_i, c_j)
```

## Calculation of Distances 
Each site calculates the encrypted distances to the current centroids, which are stored in encrypted vectors. This is done by matrix additions and multiplications in COLLAGENE.

## Calculation of Weights
Weight calculation requires exponentiation of the squared distances. Rather than using the usual approach of polynomial approximations, we used the federated exponential estimation (Implemented in *function_approximation.sh*) that masks the distance vector and collaboratively decrypts it, calculates the exponential and re-encrypts the masked distances. The masks are removed in encrypted domain.

## Centroid Coordinate Updates
This step requires an inner product between the data point coordinates and the weights. To accomplish this, we use row2row multiplication to the row expanded representation of weights after it is transposed in encrypted domain. This step demonstrates usage of row expansion for a new purpose other than matrix multiplications.

## Pooling of Results and Collaborative Normalization
The weighted coordinates from all sites are pooled to calculate a single vector coordinate for each centroid. Total weights at each site is calculated using row-row multiplication of the weights with an encrypted unit vector that is row expanded. The final step is to perform normalization of the coordinates, which requires a division operation. For this step, we use a masked division operation (Implemented in *collaborative_normalize_coords.sh*).

First, the weights are multiplicatively masked by collective mask vector from all sites (total mask vector is generated by pooling encrypted mask vectors from all sites) and they are collaboratively decrypted (Implemented in *collaborative_decrypt_matrix.sh*). Next, the mask is applied in encrypted domain to the weights matrix. 

Finally, the inverted masked weights are elementwise multiplied with the masked weights, which effectively removes the mask and normalized the coordinates.

The coordinates for K-centroids are used for the next iterations's centroids.

## Ciphertext Refreshes
We make use of ciphertext refreshes using additive masks in certain places. This is necessary as we run out of ciphertext coefficient modulus that is always tracked using COLLAGENE's ciphertext vitality statistics. Pipelines can use a similar strategy to track and refresh ciphertexts when needed (Implemented in *additive_refresh_ct.sh*). Generally the noise level of ciphertext refreshes on plaintext data is fairly small.

## Running the Code
This code uses R to generate data and make final plots. It is also necessary to copy COLLAGENE.sh and have the key directories for 3 sites (*SITE_0*, *SITE_1*, *SITE_2*) in the same directory. These can be simply copied from previous examples.

Also, make sure that the scripts are runnable and converted from dos encoding:
```
dos2unix *
chmod 755 *
```

To run the K-means testing, run following:
```
./RUN_KMEANS_PROTOCOL.sh
```

This script is the calls each step in order to generates data, do analysis, and plot the results.

There are two other scripts that it uses:
<ul>
<li> KMEANS_COMMANDS.sh : Runs the whole pipelines for all 3 sites. </li>
<li> KMEANS_UTILITIES.sh : Implements the basic processing steps at each local site. </li>
</ul>

## Limitations and Further Optimizations
We would like to re-iterate that this algorithm is for demonstration purposes, where we presented the usage of additive masking-based ciphertext refreshing and collaborative function evaluation (exponential and normalization) in the context of the federated clustering problem. The qualitative accuracy of weighted K-means algorithm is similar in essence to original K-means algorithm. However, both algorithms are very sensitive to the initial selection of K clusters. To get around this issue, weighted K-means can be executed with a large number of centroids. There are also more parameters in the weighted K-means algorithm that must be tuned for efficient run time, and numerical stability. We generally observe good concordance between plaintext and federated algorithms.

This implementation does not take the parallelization into account. For instance, the K-clusters can be fit simultaneously. In fact, multiple K-clusters can be simultaneously fit by pooling them into same ciphertext, which would enable large saving in computation. Note that this requires a slight reformulation of the matrices (not COLLAGENE).

Finally, we used the default ckks.params file that is shipped with COLLAGENE package. These parameters are likely not optimal for performance (they are 128-bit secure). 




